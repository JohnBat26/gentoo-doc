<?xml version="1.0" encoding="utf-8"?>
<article xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="linuxndi">
	<articleinfo>
		<author>
			<firstname>Артём</firstname>
			<surname>Капитула</surname>
		</author>
		<pubdate>2007</pubdate>
		<title>Linux не для идиотов</title>
		<titleabbrev>Linux не для идиотов</titleabbrev>
		<abstract>
			<para>С версии: 1.1</para>
			<para>Обновлено в версии: 2.0</para>
			<para>
				Ccылка на оригинал:
				<ulink url="http://www.myfotomx.com/linuxndi.html">myfotomx.com</ulink>
			</para>
		</abstract>
	</articleinfo>
	
	<section xml:id="overview_linuxndi">
		<info>
			<title>Предисловие</title>
		</info>
		<para>
			Я много лет работал с Linux, и, общаясь со многими единомышленниками, сделал один странный вывод: нам катастрофически не
			хватает
			документации. Причем не инструкций вида «сделайте так и вот так» и не справочных руководств, а некоторого «мостика»
			между
			новичком, который видел только графическую оболочку подобную GNOME или KDE, и профессионалом, который может
			скомпилировать
			необходимый ему драйвер, даже если этот драйвер упорно сопротивляется.</para>
		<para>Соответственно, я попытался сделать попытку написать книжку (хотя на книгу этот материал не тянет, скорей
			на методичку), которая позволила бы сравнительно просто перейти с пользовательского уровня знакомства с Linux на более высокий
			уровень, не проходя по типичным ошибкам, и за сравнительно короткое время.</para>
		<para>
			Некоторое время я использовал фрагменты этой книги также как часть учебного курса в Челябинском Государственном
			университете
			(ЧелГУ) для того, чтобы мои студенты могли ориентироваться в системе несколько лучше, чем на уровне команд
			<emphasis role="bold"> ls/ps/exit</emphasis>
			. Конечно, если честно – как учебное пособие эта маленькая книга непригодна, но,
			как мне кажется, она неплохо подходит как
			дополнительная литература.
		</para>
		<para>
			Если у вас есть пожелания и дополнения – пишите мне почтой на
			<ulink url="mailto:dalt74@gmail.com">dalt74@gmail.com</ulink>
			, я постараюсь учесть ваши замечания в следующей редакции. Большое спасибо всем тем кто участвовал в рецензировании и помогал   советами и замечаниями.
		</para>
		<para>Искренне ваш, Артем Капитула (no-dashi, dalth &amp; viking)</para>		
		<para>P.S. если вы будете распечатывать или выкладывать это пособие – пожалуйста, указывайте ссылку на автора и оригинальный источник, договорились?</para>		
		<para>P.P.S. пока что есть следующий to-do list: основы DNS, базовые настройки серверов и рабочих станций под типичные нужды. Ориентировочный срок следующей редакции – через три месяца.</para>						
	</section>
	<section xml:id="kernel_and_modules">
		<info>
			<title>Ядро и модули</title>
		</info>
		<para>
			Ядро Linux является единственным процессом, имеющим непосредственный доступ к аппаратуре – все остальные процессы обращаются к устройствам только через ядро. В ядре Linux можно выделить несколько важных подсистем: подсистему управления памятью; планировщик задач; подсистему VFS – виртуальную файловую систему и драйверы.
		</para>	
		<para>Подсистема управления памятью управляет распределением оперативной памяти между задачами, а также обслуживает файл подкачки, планировщик задач управляет разделением процессорного времени между задачами (процессами и нитями), подсистема VFS предназначена для обслуживания файловых операций.</para>
		<para>Драйверы предназначены для управления устройствами и поддержки различных протоколов. Существует две разновидности драйверов – статически подключенные в ядро драйверы и загружаемые модули; первые всегда загружены, вторые могут быть загружены при необходимости и выгружены, когда необходимость в них отпала. Каждый модуль и само ядро содержат сигнатуру версии – специальную метку, которая описывает версию ядра и некоторые опции, использованные при компиляции ядра. Кроме того, ядра версии 2.6 могут поддерживать цифровую подпись модулей. Это было сделано для повышения надежности системы – по умолчанию ядро не будет загружать и использовать драйверы, предназначенные для другой версии ядра, или собранные с другими опциями, поскольку это может привести к возникновению проблем. Версию ядра можно узнать с помощью команды <emphasis role="bold">uname</emphasis> :</para>
<screen>
$ <userinput>uname -r</userinput>
2.6.8.1
</screen>
		<para>В принципе, утилиты для работы с модулями поддерживают возможность загрузки модулей, собранных для другого ядра – но пользоваться этой возможностью следует с крайней осторожностью, поскольку это может привести к непредсказуемым последствиями – от ошибок ядра (kernel panic) и вплоть до странных потерь данных и непонятных ошибок, взявшихся на пустом месте.</para>
		<para>В большинстве дистрибутивов образ ядра располагается в каталоге /boot, а загружаемые модули ядра располагаются в /lib/modules/&lt;версия_ядра&gt;, там же располагается таблица зависимостей модулей, поскольку некоторые модули могут нуждаться для своей работы в других модулях (например, драйвер поддержки SCSI-дисков нуждается в драйвере поддержки SCSI – как следствие этого, если объекты какого-либо модуля используются другим драйвером, такой модуль невозможно выгрузить). Следующий листинг демонстрирует достаточно типичное содержание каталога модулей для ядер линейки 2.6:</para>
<screen>
$ <userinput> ls -l /lib/modules/2.6.8.1/</userinput>
total 616
lrwxrwxrwx   1 root root     18 Авг 27 15:36 build -&gt; /usr/src/linux
drwxr-xr-x  10 root root   4096 Окт  1 13:55 kernel
-rw-r--r--   1 root root 108680 Окт  1 13:56 modules.alias
-rw-r--r--   1 root root     69 Окт  1 13:56 modules.ccwmap
-rw-r--r--   1 root root 153967 Окт  1 13:56 modules.dep
-rw-r--r--   1 root root     73 Окт  1 13:56 modules.ieee1394map
-rw-r--r--   1 root root    357 Окт  1 13:56 modules.inputmap
-rw-r--r--   1 root root  16658 Окт  1 13:56 modules.isapnpmap
-rw-r--r--   1 root root  85093 Окт  1 13:56 modules.pcimap
-rw-r--r--   1 root root  68078 Окт  1 13:56 modules.symbols
-rw-r--r--   1 root root 150781 Окт  1 13:56 modules.usbmap
lrwxrwxrwx   1 root root     18 Окт  1 13:22 source -&gt; /usr/src/linux

$ <userinput>find /lib/modules/2.6.8.1/kernel -type f | head -20</userinput>		
/lib/modules/2.6.8.1/kernel/arch/i386/kernel/cpuid.ko
/lib/modules/2.6.8.1/kernel/arch/i386/kernel/microcode.ko
/lib/modules/2.6.8.1/kernel/arch/i386/kernel/msr.ko
/lib/modules/2.6.8.1/kernel/crypto/blowfish.ko
/lib/modules/2.6.8.1/kernel/crypto/deflate.ko
/lib/modules/2.6.8.1/kernel/crypto/md5.ko
/lib/modules/2.6.8.1/kernel/crypto/twofish.ko
/lib/modules/2.6.8.1/kernel/drivers/acpi/fan.ko
/lib/modules/2.6.8.1/kernel/drivers/acpi/processor.ko
/lib/modules/2.6.8.1/kernel/drivers/acpi/thermal.ko
/lib/modules/2.6.8.1/kernel/drivers/base/firmware_class.ko
/lib/modules/2.6.8.1/kernel/drivers/block/cryptoloop.ko
/lib/modules/2.6.8.1/kernel/drivers/block/loop.ko
/lib/modules/2.6.8.1/kernel/drivers/block/nbd.ko
/lib/modules/2.6.8.1/kernel/drivers/block/paride/epat.ko
/lib/modules/2.6.8.1/kernel/drivers/block/paride/paride.ko
/lib/modules/2.6.8.1/kernel/drivers/block/paride/pd.ko
/lib/modules/2.6.8.1/kernel/drivers/block/paride/pg.ko
/lib/modules/2.6.8.1/kernel/drivers/bluetooth/bcm203x.ko
/lib/modules/2.6.8.1/kernel/drivers/bluetooth/bfusb.ko
</screen>
		<para>Бинарные файлы модулей содержатся в подкаталоге kernel, и имеют расширение “.o” для ядер линейки 2.4 и расширение “.ko” для ядер линейки 2.6. В файлах modules.***map перечисляются символы (функции и переменные), экспортируемые модулями.</para>
		<para>Часто в одном каталоге с модулями содержатся ссылки на каталоги, в которых хранились исходные тексты ядра и в котором производилась сборка ядра (это ссылки source и build, соответственно). Эти ссылки, как правило, используются для того, чтобы скомпилировать модули или программы, которые зависят от версии ядра (например, эти ссылки используются при инсталляции модуля поддержки видеокарт nvidia).</para>
		<para>Учет взаимосвязей между загруженными модулями производится с помощью счетчика ссылок – модуль увеличивает свой счетчик ссылок как только какой-либо его объект начинает использоваться другими драйверами. Когда объекты модуля освобождаются, счетчик ссылок уменьшается. Модуль может быть выгружен, если число ссылок на него станет равно 0. В ядрах версии 2.6 существует возможность произвести принудительную выгрузку модуля даже если он используется, но этим пользоваться без крайней необходимости не рекомендуется, поскольку очень возможно возникновение ошибок.</para>
		<para>Ядро содержит множество переменных и функций, которые используются различными драйверами, и соответственно, если какой-либо драйвер должен обратиться к такому объекту, он должен знать его адрес. Некоторые драйверы также содержат переменные и функции, которые должны быть доступны другим драйверам, и адреса таких объектов тоже размещаются в специальной таблице. При загрузке модуля ядро и программа загрузки модулей устанавливает адреса всех объектов, в которых нуждается загружаемый модуль, и только после этого модуль может начать инициализацию.</para>
		<para>Загрузка модулей и их выгрузка осуществляются утилитами modprobe, insmod и rmmod. Программы modinfo и depmod предназначены для получения служебной информации о загружаемых модулях. В процессе своей работы эти утилиты опираются на конфигурационные файлы /etc/modprobe.conf (для ядер 2.6.X) или modules.conf (для ядер 2.4.X).</para>
	</section>
	<section xml:id="load_os">
		<info>
			<title>Загрузка операционной системы</title>
		</info>
		<para>Для компьютеров архитектуры x86 последовательность загрузки хорошо описана в специализированной литературе, но мы все-таки кратко ее повторим. После включения компьютера первым загружается BIOS. Он тестирует аппаратуру и инициализирует устройства. После этого BIOS прочитывает начальный сектор загрузочного жесткого диска (MBR), убеждается что он содержит код первичного загрузчика, и передает управление прочитанному коду. Кроме кода первичного загрузчика, начальный сектор также может содержать таблицу разделов жесткого диска.</para>
		<para>В задачи первичного загрузчика входит чтение основного кода загрузчика операционной системы и передача управления ему, после чего основная часть загрузчика может считать конфигурационный файл, загрузить ядро операционной системы, установить параметры для ядра и передать ядру управление. Ядро инициализирует драйверы, проверяет параметры и, опираясь на параметры, пытается смонтировать корневую файловую систему, после чего (если не было проинструктировано об ином) запускает программу /sbin/init. Дальнейшая работа init подробно описана во множестве книг и статей.</para>
		<para>В настоящий момент в мире Linux наиболее распространен загрузчик GRUB. Этот загрузчик состоит из нескольких частей – первичного загрузчкиа, собственно основного кода который организует интерфейс пользователя, и набора мини-драверов различных файловых систем, позволяющих прочесть необходимые файлы с файловой системы в момент когда операционная система еще недоступна. Каждая из этих компонент работает на одноим из двух этапов загрузки. Рассмотрим эти этапы:</para>
		<para>Этап 0 – здесь срабатывает первичный загрузчик GRUB. Он компактен и умещается в один блок жесткого диска, что позволяет при желании разместить его в MBR. В задачи кода stage_0 входит прочтение кода необходимого на следующем этапе (собственно кода загрузчика и мини-драйвера файловой системы где расположены основные файлы загрузчика), и передача управления прочитанному коду.</para>
		<para>Этап 1 – это на этом этапе первичным загрузчиком в память уже загружен основной код загрузчика, а также мини-драйвер файловой системы, на которой расположены конфигурационные файлы загрузчика, ядро и необходимые драйверы. Основной код, используя функции мини-драйвера, прочитывает конфигурационный файл и организует диалог с пользователем. В зависимости от выбора пользователя, используя мини-драйвер файловой системы, с диска прочитываются файлы ядра и необходимых драйверов, после чего управление передается ядру. Как вариант, пользователь может отказаться от загрузки Linux и инструктировать GRUB прочесть загрузочный сектор некоторого раздела жесткого диска и передать управление ему.</para>
		<para>Первичный загрузчик из состава GRUB может быть расположен как в MBR, так и в загрузочном секторе какого-либо раздела жесткого диска – или даже храниться в файле и быть вызван из другого загрузчика (например NTLOADER).</para>
		<para>Нередко случаются ситуации, когда корневая файловая система располагается на устройстве, чей драйвер скомпилирован в виде модуля, или драйвер корневой файловой системы скомпилирован в виде модуля. Получается замкнутый круг – чтобы смонтировать корневую файловую систему систему, необходимо прочесть драйвер, а чтобы прочесть драйвер – нужно смонтировать корневую файловую систему. Чтобы разорвать этот порочный круг, в Linux была введен поддержка initrd – INITial RamDisk.</para>
		<para>Initial ramdisk – это файл, который прочитывается загрузчиком ОС и загружается в память вместе с ядром. Ядро интерпретирует фрагмент памяти, куда загружен этот файл, как блочное устройство с помощью специального драйвера, статически вкомпилированного в ядро. После инициализации статически скомпилированных драйверов ядро монтирует файловую систему, хранящуюся в initrd и загружает с нее драйверы и запускает программы, необходимые для монтирования корневой файловой системы.</para>
		<para>Обычно файл с образом ядра хранится в каталоге /boot и называется vmlinuz-&lt;версия&gt;, там же располагается файл initrd-&lt;версия&gt;.img, содержащий образ файловой системы initrd. Для каждой версии ядра необходим свой образ initrd, в который включены модули для этой версии ядра. В большинстве случаев образ initrd поставляется в бинарном пакете вместе с ядром, или автоматически создается в процессе построения ядра из исходных текстов в момент выполнения команды make install, если же возникает ситуация, когда необходимо повторно собрать образ initrd (например, в сервере сменили SCSI-контроллер), можно воспользоваться специальной командой mkinitrd, позволяющей произвести повторную генерацию initrd:</para>
<screen>
$ <userinput> mkinitrd /tmp/initrd-2.4.8.1.img 2.6.8.1</userinput>
$ <userinput> cp /tmp/initrd-2.4.8.1.img /boot</userinput>
$ <userinput> reboot</userinput>
</screen>

		<para>Для Linux существует два основных загрузчика – LILO и GRUB. Второй является более поздней разработкой и немного удобней в
			использовании, а LILO используется по историческим или личным причинам (например, он нравится системному администратору),
			либо в некоторых случаях, когда требуются специфичные для LILO функции. Для более подробной справки лучше обратиться к
			справочному руководству (man grub, man lilo).</para>
		<para>Из интересных особенностей GRUB и LILO следует отметить то, что и оба этих загрузчика, и ядро оперируют термином корневой
			файловой системы – но если с точки зрения ядра эта та файловая система, которая содержит программу /sbin/init, то с точки зрения
			обоих загрузчиков корневой файловой системой является та, которая содержит образ ядра и файл initrd.</para>
	</section>
	
	<section xml:id="memory_organization">
		<info>
			<title>Организация памяти</title>
		</info>
		<para>Подсистема виртуальной памяти управляет распределением оперативной памяти между задачами (процессами). Каждая задача
			считает, что ей выделен непрерывный участок памяти максимального размера, поддерживаемого на соответствующей архитектуре
			(для архитектуры x86 это 4GB). Из них первый гигабайт резервируется для себя ядром, второй отдается под код программы и
			разделяемые библиотеки (оба этих фрагмента ядром защищаются), а два последних гигабайта отдаются собственно программе под
			ее данные – но это только то, как видит это все программа.</para>
		<para>На самом же деле программа занимает только тот объем памяти, с которым она реально работает. Большинство памяти существует
			только “на бумаге”, т.е. будет предоставлена программе в тот момент, когда она обратится в эту область. Ядро распределяет память
			страницами фиксированного размера. Процедура, когда страница оперативной памяти объявляется частью адресного пространства
			процесса, называется отображением этой страницы в адресное пространство процесса.</para>
		<para>Соответственно, ядро отображает реально используемые страницы в виртуальное адресное пространство процесса. Когда процесс
			обращается к некоторой странице своего адресного пространства, ядро проверяет, имеет ли он право на доступ к этой странице, и
			если проверка пройдена и доступ получен, то ядро переадресовывает обращение на реальный адрес этой страницы. Если это первое
			обращение к странице, ядро попытается найти свободную страницу и в случае успеха отобразит ее в адресное пространство
			соответствующего процесса. Размер страницы фиксирован архитектурой процессора, и для x86 ее размер составляет 4096
			байт.</para>
		<para>Если случается ситуация, когда свободных страниц больше нет, но существует файл подкачки, ядро может убрать одну из
			наиболее долго не использовавшихся страниц в файл подкачки, и освободившуюся физическую страницу отдать запросившему
			память процессу. Если же нет ни незанятого пространства в файле подкачки, ни свободных страниц RAM, то развитие событий может
			быть следующим: либо запросивший память процесс прерван и “убит” системой, либо какой-то другой из процессов (это
			определяется специфическими алгоритмами) будет “убит” ядром, и освободившаяся память будет передана запросившему память
			процессу.</para>
		<para>На самом деле большинством действий занимается одна из подсистем процессора, называемая MMU – Memory Management Unit, и
			в действительности ядро просто полагается на его работу и вмешивается в нее только для проведения операций пейджинга
			(подгрузки/выгрузки страниц в SWAP-файл), или когда возникает ошибка доступа к странице.</para>
		<para>Ограничение адресного пространства в 4GB не означает, что система не сможет адресовать более этого объема памяти. На
			платформе x86 ядро Linux может использовать до 64GB, а ограничение в 4GB накладывается лишь на размер адресного
			пространства процесса.</para>
	</section>
	<section xml:id="system_5_shared_memory">
		<info>
			<title>System V shared memory</title>
		</info>
		<para>Linux поддерживает стандартную для всех UNIX-подобных операционных систем организацию разделяемой памяти. Пользовательские приложения могут создавать сегменты разделяемой памяти, которые могут быть присоединены к некоторому фрагменту адресного пространства процесса. Любой процесс, имеющий достаточные права доступа, может присоединиться к сегменту разделяемой памяти, и отобразить его в свое адресное пространство, начиная с некоторого адреса.</para>
		
		<para>Если в приведенной схеме любой из процессов изменит содержимое памяти в области, занимаемой отображением одного из сегментов, то же самое изменение произойдет в адресном пространстве другого процесса, поскольку соответствующий сегмент существует в одном экземпляре и отображен в адресное пространство обоих процессов.</para>
		<para>Кроме System V IPC ядро Linux также поддерживает другие объекты IPC, в частности семафоры и очереди сообщений. Каждый объект System V IPC идентифицируется уникальным ключом. Просмотреть список всех объектов IPC можно командой ipcs. Команда ipcrm позволяет удалять объекты IPC, которые по каким-либо причинам остались не освобожденными после завершения создавшего их процесса – например, такая ситуация может возникнуть после аварийного завершения работы СУБД Oracle, Informix или DB2.</para>
		<para>Соответственно, перед перезапуском процесса системный администратор с помощью команды ipcrm должен освободить неиспользуемые объекты IPC, поскольку стартующее приложение не сможет их повторно создать и не будет корректно работать.</para>
		<para>Для каждого объекта IPC система устанавливает права доступа, как если бы это был файл (т.е. для каждого объекта IPC можно устанавливать набор прав ugo/rwx, но в отличие от обычных файлов сменить права доступа для IPC-объектов можно только вызывая специализированные функции, предназначенные для работы с такими объектами.</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="images/System_V_shared_memory.png" format="PNG"  width="60%" align="center"/>
			</imageobject>
			<textobject>
				<phrase>System V shared memory</phrase>	 
			</textobject>
			<caption>
				<para><emphasis role="strong">System V shared memory.</emphasis></para>
			</caption>
    		</mediaobject>
		<screen>
$ <userinput> ipcs</userinput>

------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status      
0x00000000 0          oracle    640        4194304    10                      
0x00000000 32769      oracle    640        20971520   10                      
0x00000000 65538      oracle    640        29360128   10                      
0x0d3c24a0 98307      oracle    640        29360128   50                      
0x00000000 13697028   root      777        49152      1                       
0x00000000 13729797   root      777        16384      1                       
0x000004d2 13795334   dalth     666        1008       2                       
0x00000000 14286866   root      644        790528     2          dest         
0x00000000 21823507   dalth     600        393216     2          dest         
0x00000000 21921814   root      644        122880     2          dest         
0x00000000 14516249   root      644        151552     1          dest         

------ Semaphore Arrays --------
key        semid      owner      perms      nsems     
0x0b4f657c 262147     oracle    640        154       
0x000004d2 458756     dalth     666        1         

------ Message Queues --------
key        msqid      owner      perms      used-bytes   messages 
</screen>
<para>Поддержка System V IPC позволяет сравнительно легко переносить на Linux приложения, написанные для других UNIX-систем.</para>
	</section>
	<section xml:id="file_system">
		<info>
			<title>Файловая система</title>
		</info>
		<para>VFS и драйверы файловых систем являются одной из важнейших составляющих ядра. Для того, чтобы получить доступ к файлам,
			хранящимся на каком-либо устройстве хранения данных, необходимо, чтобы в ядро был загружен драйвер соответствующей
			файловой системы, и файловая система была смонтирована. Драйвера всех файловых систем поддерживают набор стандартных
			функций: открыть файл по имени, записать данные в файл, прочитать данные из файла, закрыть файл, удалить файл и т.д.</para>
		
		<para>Уточним, что драйвера файловых систем не занимаются кэшированием, этим занимается VFS.</para>
		<para>При первоначальной загрузке драйвер файловой системы регистрирует в VFS имя файловой системы и те функции, которые
			предназначены для выполнения стандартных файловых операций. Впоследствии при обращении к файлу на какой-либо файловой
			системе VFS будет переадресовывать обращение на соответствующую функцию, если таковая была зарегистрирована драйвером.
			Посмотреть список обслуживаемых ядром файловых систем можно в файле /proc/filesystems:</para>
<screen>
$ <userinput> cat /proc/filesystems </userinput>
nodev   sysfs
nodev   rootfs
nodev   bdev
nodev   proc
nodev   sockfs
nodev   usbfs
nodev   usbdevfs
nodev   futexfs
nodev   tmpfs
nodev   pipefs
nodev   eventpollfs
nodev   devpts
ext2
nodev   ramfs
nodev   hugetlbfs
iso9660
nodev   devfs
nodev   mqueue
ext3
nodev   rpc_pipefs
nodev   nfsd
nodev   smbfs
</screen>
		<para>Операция монтирования предназначена для того, чтобы сделать доступной файловую систему, расположенную на каком-либо
			блочном устройстве. Суть операции монтирования заключается в том, что ядро ассоциирует некоторый каталог (называемый точкой 
			монтирования) с блочным устройством и драйвером файловой системы. Для этого оно передает ссылку на блочное устройство
			драйверу файловой системы, и в случае, если драйвер успешно проидентифицировал эту файловую систему, ядро заносит в
			специальную таблицу монтирования информацию о том, что все файлы и каталоги, чей полный путь начинается с указанной точки
			монтирования, обслуживаются соответствующим драйвером файловой системы и расположены на указанном блочном
			устройстве.</para>
		<para>Некоторые файловые системы не нуждаются в блочном устройстве, поскольку хранят свои данные исключительно в памяти,
			например файловая система procfs, через файлы которой можно получить доступ к различным системным параметрам и
			таблицам.</para>
		<para>Очень часто при монтировании файловой системы системный администратор имеет возможность задать опции монтирования.
			Опции монтирования – это специальные параметры, которые влияют на работу драйвера файловой системы, когда он работает с
			файловой системой на соответствующем блочном устройстве – например, с помощью опций монтирования можно управлять режимом
			кэширования данных, преобразованиями имен файлов и данных, включать и отключать поддержку ACL и т.д.</para>
		<para>Посмотреть таблицу примонтированных файловых систем можно через файл /proc/mounts:</para>
<screen>
$<userinput> cat /proc/mounts </userinput>
rootfs / rootfs rw 0 0
/dev/root / ext3 rw 0 0
none /dev devfs rw 0 0
/proc /proc proc rw,nodiratime 0 0
/sys /sys sysfs rw 0 0
none /dev/pts devpts rw 0 0
usbdevfs /proc/bus/usb usbdevfs rw 0 0
/dev/chimera/var /var ext3 rw 0 0
/dev/chimera/temp /tmp ext3 rw 0 0
/dev/chimera/usr /usr ext3 rw 0 0
/dev/chimera/home /home ext3 rw 0 0
/dev/chimera/opt /opt ext3 rw 0 0
none /dev/shm tmpfs rw 0 0
</screen>
	<para>Виртуальная файловая система Linux различает несколько типов файлов: каталоги, обычные файлы, именованные каналы,
		символьные ссылки, сокеты и специальные файлы. Каждая из этих разновидностей обрабатывается своим собственным образом:</para>
<orderedlist>
	<listitem>
		<para>Обычные файлы могут быть прочитаны, записаны, удалены или отображены в память.</para>
	</listitem>
	<listitem>
		<para>Каталог можно представить как список имен файлов, и для этого списка определены операции добавления элемента в список,
			удаление элемента из списка, переименование элемента списка.</para>
	</listitem>
	<listitem>
		<para>Именованные каналы являются просто буферами – в него можно записать некоторый объем данных, и прочесть их в том же
			порядке, в каком они были записаны.</para>
	</listitem>
	<listitem>
		<para>Сокеты являются вариантом именованных каналов, но если у именованного канала буфер только один, то есть нельзя определить
			какой процесс записал данные в канал, то у сокетов может быть несколько клиентов, один из которых осуществляет управление
			сокетом и может обмениваться данными с любым из остальных клиентов, а те в свою очередь могут обмениваться данными с
			диспетчером канала – то есть сокет поддерживает множество независимых буферов, по одному на каждую пару
			сервер+клиент</para>
	</listitem>
	<listitem>
		<para>Специальные файлы предназначены для того, чтобы осуществлять прямой доступ к устройствам. Подробнее о специальных
			файлах будет говориться в главе Секреты /dev</para>
	</listitem>
	<listitem>
		<para>Символьные ссылки являются “ярлыками”, которые могут содержать имя другого файла – и тогда операции чтения, записи и
			открытия/закрытия файла автоматически переадресовываются к файлу, на который указывает символьная ссылка, но в отличие от
			“ярлыков” Windows (shortcuts), символьные ссылки (symbolic links) не требуют специальных функций при работе – все программы
			(кроме тех, которые специально предназначены для работы с ними) видят их как обычные файлы, и также их открывают, читают и
			записывают данные и т.д.</para>
	</listitem>
</orderedlist>
		<para>В некоторых файловых системах, которые изначально проектировались для UNIX-подобных систем, есть возможность создавать
			кроме символьных ссылок еще и жесткие ссылки. Фактически, жесткая ссылка – это второе имя для файла. Жесткие ссылки
			возможно создавать только в пределах одной файловой системы.</para>
		<para>Из-за того, что в VFS присутствует понятие кэширования, перед отключением системы необходимо делать обязательный сброс
			изменений на диск. Сброс кэша на диск осуществляется в момент размонтирования файловой системы. Кроме того, с помощью
			команды	sync можно в любой момент принудительно сбросить на диск все закэшированные изменения в файловой системе
			(например, системные администраторы часто делают sync перед загрузкой нового драйвера). Размонтировать файловую систему
			можно только тогда, когда ни один процесс не удерживает в открытом состоянии файлов с этой файловой системы, а также не
			находится ни один процесс не имеет рабочим каталогом каталога с размонтируемой файловой системы. При невыполнении этого
			условия размонтировать файловую систему не удастся:</para>
<screen>
$<userinput> umount /home/ftp/pub/linux/fedora/cd1 </userinput>
umount: /home/ftp/pub/linux/fedora/cd1: device is busy
umount: /home/ftp/pub/linux/fedora/cd1: device is busy
</screen>
		<para>Некоторые файловые системы поддерживают специальные опции, позволяющие принудительно синхронизировать файловую
			систему при каждой операции чтения или записи. Обычно опции, влияющие на синхронизацию файловой системы, содержат в
			своем названии слово sync, например приведенная ниже команда инструктирует операционную систему примонтировать некоторый
			раздел в режиме принудительной синхронизации:</para>
<screen>
$<userinput> mount -t ext3 -o sync,dirsync /dev/hda9 /home </userinput>
</screen>
		<para>Следует учесть, что принудительная синхронизация – это удар по производительности операций записи для файловой системы,
			смонтированной в таком режиме, поэтому использовать такой его следует осторожно.</para>
	</section>
	<section xml:id="privileges">
		<info>
			<title>Права доступа</title>
		</info>
		<para>Кроме стандартных наборов прав доступа к файлам некоторые файловые системы Linux поддерживают т.н. POSIX ACL – списки
			контроля доступа POSIX. Эта возможность позволяет гибко управлять доступом к файлу, не ограничиваясь “классическим” набором
			ugo/rwx. Для того, чтобы использовать на файловой системе POSIX ACL, необходимо смонтировать файловую систему с опцией
			acl:</para>
<screen>
$<userinput>mount -t ext3 -o acl /dev/inferno/opt /opt</userinput>
</screen>
		<para>Возможно также настроить соответствующий параметр для файловой системы, чтобы она поддерживала POSIX ACL по
			умолчанию:</para>
<screen>
$<userinput> tune2fs -o acl /dev/inferno/opt</userinput>
</screen>
		<para>После установки соответствующей опции можно приступать к работе с POSIX ACL. Для работы с ними существует две базовых
		утилиты: getfacl для получения списка дополнительных атрибутов доступа, и setfacl для установки расширенных атрибутов контроля
		доступа. Если в выводе команды ls вы видите символ “+” рядом со списком стандартных прав доступа, это означает, что для файла также
		установлены 	расширенные атрибуты контроля доступа:</para>
<screen>
$<userinput> ls -l /home/dalth/.bash_???????</userinput>
-rw-r-----+ 1 dalth dalth 20034 Окт 11 22:48 /home/dalth/.bash_history
-rw-r--r--  1 dalth dalth   191 Авг 23 21:51 /home/dalth/.bash_profile
</screen>
		<para>Для просмотра значений расширенных атрибутов можно воспользоваться утилитой getfacl. Жирным шрифтом выделен
			дополнительный атрибут контроля доступа, позволяющий пользователю kiki получить доступ на чтение к файлу
			.bash_history:</para>
<screen>
$<userinput>getfacl .bash_history</userinput>
# file: .bash_history
# owner: dalth
# group: dalth
user::rw-
user:kiki:r--
group::---
mask::r--
other::---
</screen>
		<para>Добавим пользователю oracle права на чтение и запись файла .bash_history с помощью команды setfacl, и затем отберем
			дополнительные права на доступ к указанному файлу у пользователя kiki:</para>
<screen>
$<userinput>setfacl -m u:oracle:rw .bash_history</userinput>
$<userinput>setfacl -x u:kiki .bash_history</userinput>
$<userinput>getfacl .bash_history</userinput>
# file: .bash_history
# owner: dalth
# group: dalth
user::rw-
user:oracle:rw-
group::---
mask::rw-
other::---
</screen>
		<para>Последним шагом сбросим все расширенные атрибуты с файла с файла .bash_history:</para>
<screen>
$<userinput>setfacl -b .bash_history</userinput>
$<userinput>ls -l .bash_history</userinput>
$<userinput>getfacl .bash_history</userinput>
-rw-------  1 dalth dalth 20034 Окт 11 22:48 .bash_history
</screen>
		<para>Расширенные атрибуты позволяют гибко контролировать доступ к файловых объектам, обходя стратегию ugo/rwx пришедшую из
			“классического UNIX”. Права доступ на файловые объекты могут быть выданы не только пользователю, но и группе.</para>
		<para>К сожалению, далеко не все утилиты и файловые системы поддерживают ACL, поэтому при резервном копировании или
			восстановлении файлов необходимо проверять корректность установки расширенных атрибутов и правильность их
			переноса.</para>
	</section>
	<section xml:id="journal_file_system">
		<info>
			<title>Журналируемые файловые системы</title>
		</info>
		<para>Для обеспечения сохранности данных и обеспечения целостности файловых систем при неожиданных сбоях были разработаны
			журналируемые файловые системы. Как правило, у этих файловых систем существует специальная область данных, называемая
			журналом. Все изменения, которые необходимо произвести с файловой системой, сначала записываются в журнал, и уже из
			журнала попадают в основную часть файловой системы.</para>
		<para>В большинстве случаев журналируются только метаданные файловых систем (служебная информация, обеспечивающая
			целостность структуры файловой системы – например, изменения в каталогах или служебных таблицах размещения файлов).
			Некоторые файловые системы позволяют журналировать не только метаданные, но и данные файлов – такой шаг позволяет
			повысить надежность, но уменьшает скорость записи данных на файловую систему, поскольку каждый блок данных записывается
			на диск дважды – сначала в журнал, и затем из журнала в основную область файловой системы.</para>
		<para>В большинстве случаев, журналируемые файловые системы способны решить проблемы с надежностью при неожиданных сбоях
			без тех потерь производительности, к которым может привести использование опции sync при монтировании.</para>
		<para>В частности, к журналируемым файловым системам, например, относятся EXT3, ReiserFS, XFS, JFS и некоторые другие.</para>
	</section>
	<section xml:id="mapped_memory_files">
		<info>
			<title>Отображаемые в память файлы</title>
		</info>
		<para>Объединение кэширования файлов и разделяемой памяти позволяет реализовать такое действие, как отображение файла в
			память. Для простоты можно представить, что файл загружается в кэш, и страницы кэша отображаются в адресное пространство
			процесса, и в результате любое изменение в том фрагменте адресного пространства, которое занято отображением файла,
			автоматически попадает в закэшированные данные файла. Когда файл закрывается, закэшированные изменения сбрасываются на
			диск, изменяя сам файл. Кроме того, в свободное время ядро также постепенно сбрасывает изменившиеся кэшированные данные на
			диск.</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="images/mapped_memory_files.png" format="PNG"  width="60%" align="center"/>
			</imageobject>
			<textobject>
				<phrase>Files mapped in memory </phrase>	 
			</textobject>		
    		</mediaobject>
		<para>На самом деле, механизм отображения файлов в память куда “хитрее” - при обращении по записи к странице, которая является
			отображением некоторого файла, ядро перехватывает обращение, производит запись в файл (в подавляющем большинстве случаев
			эта операция попадает в кэш). При обращении по чтению к такой странице ядро опять же перехватывает обращение и производит
			чтение из файла – в большинстве случаев это чтение производится из кэша. Для наших же целей проще будет считать, что страницы
			кэша отображены в память процесса.</para>
		<para>Такая методика часто используется для загрузки разделяемых библиотек, когда выполняемый код библиотек и исполняемого кода
			программы хранится в кэше и через отображение файла в память становится виден в адресном пространстве процесса:</para>
<screen>
$<userinput>cat /proc/1/maps</userinput>
08048000-08050000 r-xp 00000000 03:01 75813      /sbin/init
08050000-08051000 rw-p 00008000 03:01 75813      /sbin/init
08051000-08072000 rw-p 08051000 00:00 0 
40015000-40016000 rw-p 40015000 00:00 0 
4c8ee000-4c903000 r-xp 00000000 03:01 92869      /lib/ld-2.3.3.so
4c903000-4c904000 r--p 00014000 03:01 92869      /lib/ld-2.3.3.so
4c904000-4c905000 rw-p 00015000 03:01 92869      /lib/ld-2.3.3.so
4c907000-4ca1c000 r-xp 00000000 03:01 92857      /lib/tls/libc-2.3.3.so
4ca1c000-4ca1e000 r--p 00115000 03:01 92857      /lib/tls/libc-2.3.3.so
4ca1e000-4ca20000 rw-p 00117000 03:01 92857      /lib/tls/libc-2.3.3.so
4ca20000-4ca22000 rw-p 4ca20000 00:00 0 
4d201000-4d20f000 r-xp 00000000 03:01 92965      /lib/libselinux.so.1
4d20f000-4d211000 rw-p 0000d000 03:01 92965      /lib/libselinux.so.1
bfffd000-c0000000 rw-p bfffd000 00:00 0 
ffffe000-fffff000 ---p 00000000 00:00 0 
</screen>
		<para>На самом деле, драйверы любого устройства и любой файловой системы могут по-своему реализовывать операцию mmap, но
			большинство драйверов файловых систем полагаются в этом на VFS.</para>
	</section>
	<section xml:id="special_file_systems">
		<info>
			<title>Специальные файловые системы</title>
		</info>
		<para>Некоторые типы файловых систем являются специальными и предназначаются для выполнения или реализации специфических
			задач операционной системы. К таким файловым системам относятся файловые системы procfs и sysfs, предоставляющие доступ к
			различным параметрам системы и системным объектам, “живущим” в ядре.</para>
		<para>Файловая система sysfs в основном предоставляет доступ к объектам ядра и отображает взаимосвязи между ними. Файловая
			система procfs предоставляет доступ к различным параметрам ядра и драйверов и к пользовательским процессам, позволяя тем
			самым реализовать такие команды как ps или sysctl. Большинство файлов в sysfs двоичные, в procfs – текстовые.</para>
		<para>Драйверы файловых системы ramfs, tmpfs и shmfs очень похожи, и после монтирования файловой системы такого типа на ней
			можно создавать файлы, хранящиеся в памяти и отличаются в основном небольшими особенностями работы (например, страницы,
			используемые ramfs под данные файлов, не вытесняются в swap-файл в отличие от shmfs и tmpfs). В ядре 2.6 shmfs была заменена
			на tmpfs.</para>
	</section>
	<section xml:id="network_file_systems">
		<info>
			<title>Сетевые файловые системы</title>
		</info>
		<para>Сетевые файловые системы предназначены для получения доступа к файловым системам других компьютеров с использованием
			сетевых протоколов.</para>
		<para>Наиболее часто используются сетевые файловые системы NCPFS (для доступа к серверам Novell NetWare), SMBFS (для доступа к
			серверам Windows) и NFS (для доступа к файловым системам других UNIX-систем).</para>
		<para>Как правило, процедура монтирования сетевых файловых систем схожа с процедурой монтирования обычных файловых систем
			на блочных устройствах с тем отличием, что вместо блочного устройства указывается адрес сервера, чья файловая система
			монтируется, и имя монтируемого ресурса. Для примера рассмотрим процедуры монтирования ресурсов, доступных по SMB и по
			NFS:</para>
<screen>
$<userinput>mount -t smbfs -o username=usr,workgroup=tst //server/share_name /mnt/smb_target</userinput>
Password: ********
$<userinput>mount -t nfs -o timeout=4 server:/export/home /mnt/nfs_target</userinput>
</screen>
		<para>В данном примере опция -t команды mount указывает тип файловой системы, опция -o позволяет задать дополнительные
			параметры для монтирования – для SMB мы задаем, например, имя пользователя, с правами которого производится подключение к
			серверу и имя рабочей группы или домена, для NFS мы указываем таймаут, по истечении которого операция ввода/вывода считается
			неудавшейся. Вместо блочного устройства мы указываем адрес сервера, ресурс которого хотим использовать, и имя ресурса на
			сервере. Последним параметром идет точка монтирования.</para>				
	</section>
</article>